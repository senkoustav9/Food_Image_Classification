{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMBmoyYTuDU"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhBcczgJFsay",
        "outputId": "23cf77e2-2a87-434e-f25f-54789d72d44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "#code to mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UocrO8hOFsaz"
      },
      "source": [
        "## Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KGcEYRNtS6mo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLqBh4nJFsa1"
      },
      "source": [
        "## Gathering Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OWT3JkoT7qH"
      },
      "outputs": [],
      "source": [
        "(train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS\n",
        "                                             split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n",
        "                                             shuffle_files=False, # shuffle files on download?\n",
        "                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n",
        "                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QNpZWGvFsa4"
      },
      "source": [
        "## Accessing & Visualizing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaX8ZxxoT7tH",
        "outputId": "e64b8273-e5a5-4015-c972-db01fe9a2a3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FeaturesDict({\n",
              "    'image': Image(shape=(None, None, 3), dtype=uint8),\n",
              "    'label': ClassLabel(shape=(), dtype=int64, num_classes=101),\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Features of Food101 TFDS\n",
        "ds_info.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ii4X-EGT7wt",
        "outputId": "a0732a06-d66b-4123-bce0-eb030df2e709"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get class names\n",
        "class_names = ds_info.features[\"label\"].names\n",
        "train_one_sample = train_data.take(1)\n",
        "train_one_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qciw9MglU0YQ",
        "outputId": "68368a3c-25a6-438a-8d43-7b064a41f75a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Image shape: (512, 512, 3)\n",
            "  Image dtype: <dtype: 'uint8'>\n",
            "  Target class from Food101 (tensor form): 56\n",
            "  Class name (str form): huevos_rancheros\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Output info about our training sample\n",
        "for image, label in train_one_sample:\n",
        "  print(f\"\"\"\n",
        "  Image shape: {image.shape}\n",
        "  Image dtype: {image.dtype}\n",
        "  Target class from Food101 (tensor form): {label}\n",
        "  Class name (str form): {class_names[label.numpy()]}\n",
        "  \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkbCDnROFsa9"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac9p-UZsVSqW"
      },
      "outputs": [],
      "source": [
        "# Make a function for preprocessing images\n",
        "def preprocess_img(image, label, img_shape=224):\n",
        "  image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape\n",
        "  return tf.cast(image, tf.float32), label # return (float32_image, label) tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ozpj8BvVSuO",
        "outputId": "c557e360-6c99-40a3-b751-6dc482b837be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (224, 224, 3),\n",
            "Datatype: <dtype: 'float32'>\n"
          ]
        }
      ],
      "source": [
        "# Preprocess a single sample image and check the outputs\n",
        "preprocessed_img = preprocess_img(image, label)[0]\n",
        "print(f\"Shape: {preprocessed_img.shape},\\nDatatype: {preprocessed_img.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYEbBhZFVS2j"
      },
      "outputs": [],
      "source": [
        "# Map preprocessing function to training data (and paralellize)\n",
        "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
        "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Map prepreprocessing function to test data\n",
        "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Turn test data into batches (don't need to shuffle)\n",
        "test_data = test_data.batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYTTn7mTWEMk",
        "outputId": "9bb95e56-d5e0-45a8-f8a0-8eb5d0eecfb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n",
              " <_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data, test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYt-x9XJFsa_"
      },
      "source": [
        "## Create modelling callbacks & Setting mixed precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HPqR0urWERT"
      },
      "outputs": [],
      "source": [
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  \"\"\"\n",
        "  Creates a TensorBoard callback instand to store log files.\n",
        "  Stores log files with the filepath:\n",
        "    \"dir_name/experiment_name/current_datetime/\"\n",
        "  Args:\n",
        "    dir_name: target directory to store TensorBoard log files\n",
        "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
        "  \"\"\"\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n",
        "\n",
        "checkpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      montior=\"val_accuracy\", # save the model weights with best validation accuracy\n",
        "                                                      save_best_only=True, # only save the best weights\n",
        "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
        "                                                      verbose=1) # don't print out whether or not model is being saved\n",
        "\n",
        "# Turn on mixed precision training\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy(policy=\"mixed_float16\") # set global policy to mixed precision\n",
        "mixed_precision.global_policy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctYXx4TRFsbA"
      },
      "source": [
        "## Build feature extraction model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F_OZJNnXDTL",
        "outputId": "62f5ea32-3819-4a24-d507-eea83f143cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Create base model\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False # freeze base model layers\n",
        "\n",
        "# Create Functional model\n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\", dtype=tf.float16)\n",
        "x = base_model(inputs, training=False) # set base_model to inference mode only\n",
        "x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
        "x = layers.Dropout(.3)(x)\n",
        "x = layers.Dense(len(class_names))(x) # want one output neuron per class\n",
        "\n",
        "# Separate activation of output layer so we can output float32 activations\n",
        "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpAVxCbnXJoj"
      },
      "outputs": [],
      "source": [
        "# Check out our model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVE-alTyX5jl",
        "outputId": "620a6482-8424-49b3-b8f7-a729a97e5bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientnetb0_101_classes_all_data_feature_extract/20231112-055447\n",
            "Epoch 1/5\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 1.8487 - accuracy: 0.5506\n",
            "Epoch 1: val_loss improved from inf to 1.16716, saving model to model_checkpoints/cp.ckpt\n",
            "2368/2368 [==============================] - 253s 96ms/step - loss: 1.8487 - accuracy: 0.5506 - val_loss: 1.1672 - val_accuracy: 0.6954\n",
            "Epoch 2/5\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 1.3651 - accuracy: 0.6460\n",
            "Epoch 2: val_loss improved from 1.16716 to 1.06901, saving model to model_checkpoints/cp.ckpt\n",
            "2368/2368 [==============================] - 211s 87ms/step - loss: 1.3651 - accuracy: 0.6460 - val_loss: 1.0690 - val_accuracy: 0.7100\n",
            "Epoch 3/5\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 1.2595 - accuracy: 0.6709\n",
            "Epoch 3: val_loss improved from 1.06901 to 1.02822, saving model to model_checkpoints/cp.ckpt\n",
            "2368/2368 [==============================] - 198s 82ms/step - loss: 1.2595 - accuracy: 0.6709 - val_loss: 1.0282 - val_accuracy: 0.7225\n",
            "Epoch 4/5\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 1.2014 - accuracy: 0.6830\n",
            "Epoch 4: val_loss improved from 1.02822 to 1.00726, saving model to model_checkpoints/cp.ckpt\n",
            "2368/2368 [==============================] - 197s 82ms/step - loss: 1.2014 - accuracy: 0.6830 - val_loss: 1.0073 - val_accuracy: 0.7235\n",
            "Epoch 5/5\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 1.1653 - accuracy: 0.6898\n",
            "Epoch 5: val_loss improved from 1.00726 to 0.99621, saving model to model_checkpoints/cp.ckpt\n",
            "2368/2368 [==============================] - 202s 84ms/step - loss: 1.1653 - accuracy: 0.6898 - val_loss: 0.9962 - val_accuracy: 0.7278\n"
          ]
        }
      ],
      "source": [
        "# Fit the model with callbacks\n",
        "history_feature_extract = model.fit(train_data,\n",
        "                                    epochs=5,\n",
        "                                    steps_per_epoch=len(train_data),\n",
        "                                    validation_data=test_data,\n",
        "                                    validation_steps=int(0.15 * len(test_data)),\n",
        "                                    callbacks=[create_tensorboard_callback(\"training_logs\",\n",
        "                                                                           \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
        "                                               model_checkpoint]\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX-bdnNoX5nL",
        "outputId": "e0891b4c-8814-4f21-85ad-c3fd32c8e36d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "790/790 [==============================] - 56s 71ms/step - loss: 1.0080 - accuracy: 0.7255\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.007992148399353, 0.7255445718765259]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate model (unsaved version) on whole test dataset\n",
        "results_feature_extract_model = model.evaluate(test_data)\n",
        "results_feature_extract_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opilPywWX5te",
        "outputId": "98573186-5b19-49dd-ee4e-c0eaf3072a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoint  cp.ckpt.data-00000-of-00001  cp.ckpt.index\n"
          ]
        }
      ],
      "source": [
        "!ls model_checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GllUY_YGYz_h",
        "outputId": "13b3aab8-0d11-4ca1-e517-28fcc802efa2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Create save path to drive\n",
        "save_dir = \"drive/MyDrive/food_app/extractor/\"\n",
        "os.makedirs(save_dir) # Make directory if it doesn't exist\n",
        "\n",
        "# Save model\n",
        "model.save(save_dir)\n",
        "model.save(\"/content/drive/MyDrive/food_app/ExtractorModel.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggGoUhtoZ4dC"
      },
      "outputs": [],
      "source": [
        "loaded_saved_model = tf.keras.models.load_model(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icEwOYPvFsbF"
      },
      "source": [
        "## Preparing our model's layers for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF7uFMoZdAQy"
      },
      "outputs": [],
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "checkpoint_path = \"fine_tune_checkpoints/\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      save_best_only=True,\n",
        "                                                      monitor=\"val_loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKGEVQCvcdEf"
      },
      "outputs": [],
      "source": [
        "# Creating learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhJgBfaQdhps"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "loaded_saved_model.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n",
        "                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n",
        "                        metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdgeCauhdhsq",
        "outputId": "b2fda253-d9a4-448c-fa69-1bfd729a7736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientb0_101_classes_all_data_fine_tuning/20231112-061535\n",
            "Epoch 1/20\n",
            "2368/2368 [==============================] - 261s 105ms/step - loss: 1.0577 - accuracy: 0.7169 - val_loss: 0.9667 - val_accuracy: 0.7360 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "2368/2368 [==============================] - 252s 104ms/step - loss: 1.0471 - accuracy: 0.7184 - val_loss: 0.9625 - val_accuracy: 0.7365 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "2368/2368 [==============================] - 253s 105ms/step - loss: 1.0440 - accuracy: 0.7199 - val_loss: 0.9609 - val_accuracy: 0.7338 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "2368/2368 [==============================] - 252s 104ms/step - loss: 1.0375 - accuracy: 0.7216 - val_loss: 0.9594 - val_accuracy: 0.7338 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "2368/2368 [==============================] - 257s 107ms/step - loss: 1.0349 - accuracy: 0.7225 - val_loss: 0.9569 - val_accuracy: 0.7360 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "2368/2368 [==============================] - 251s 105ms/step - loss: 1.0296 - accuracy: 0.7233 - val_loss: 0.9562 - val_accuracy: 0.7370 - lr: 1.0000e-04\n",
            "Epoch 7/20\n",
            "2368/2368 [==============================] - 263s 110ms/step - loss: 1.0244 - accuracy: 0.7250 - val_loss: 0.9548 - val_accuracy: 0.7386 - lr: 1.0000e-04\n",
            "Epoch 8/20\n",
            "2368/2368 [==============================] - 251s 104ms/step - loss: 1.0225 - accuracy: 0.7252 - val_loss: 0.9542 - val_accuracy: 0.7394 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "2368/2368 [==============================] - 246s 103ms/step - loss: 1.0157 - accuracy: 0.7259 - val_loss: 0.9517 - val_accuracy: 0.7386 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "2368/2368 [==============================] - 259s 108ms/step - loss: 1.0174 - accuracy: 0.7255 - val_loss: 0.9494 - val_accuracy: 0.7407 - lr: 1.0000e-04\n",
            "Epoch 11/20\n",
            "2368/2368 [==============================] - 199s 83ms/step - loss: 1.0137 - accuracy: 0.7274 - val_loss: 0.9500 - val_accuracy: 0.7383 - lr: 1.0000e-04\n",
            "Epoch 12/20\n",
            "2367/2368 [============================>.] - ETA: 0s - loss: 1.0105 - accuracy: 0.7278\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "2368/2368 [==============================] - 202s 84ms/step - loss: 1.0106 - accuracy: 0.7278 - val_loss: 0.9503 - val_accuracy: 0.7383 - lr: 1.0000e-04\n",
            "Epoch 13/20\n",
            "2368/2368 [==============================] - 250s 104ms/step - loss: 0.9995 - accuracy: 0.7294 - val_loss: 0.9489 - val_accuracy: 0.7399 - lr: 2.0000e-05\n",
            "Epoch 14/20\n",
            "2368/2368 [==============================] - 252s 106ms/step - loss: 1.0001 - accuracy: 0.7330 - val_loss: 0.9484 - val_accuracy: 0.7399 - lr: 2.0000e-05\n",
            "Epoch 15/20\n",
            "2368/2368 [==============================] - 252s 105ms/step - loss: 0.9974 - accuracy: 0.7311 - val_loss: 0.9483 - val_accuracy: 0.7402 - lr: 2.0000e-05\n",
            "Epoch 16/20\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 0.9967 - accuracy: 0.7300\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "2368/2368 [==============================] - 212s 88ms/step - loss: 0.9967 - accuracy: 0.7300 - val_loss: 0.9485 - val_accuracy: 0.7399 - lr: 2.0000e-05\n",
            "Epoch 17/20\n",
            "2368/2368 [==============================] - 214s 89ms/step - loss: 0.9931 - accuracy: 0.7323 - val_loss: 0.9484 - val_accuracy: 0.7397 - lr: 4.0000e-06\n",
            "Epoch 18/20\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 0.9940 - accuracy: 0.7316\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
            "2368/2368 [==============================] - 198s 82ms/step - loss: 0.9940 - accuracy: 0.7316 - val_loss: 0.9484 - val_accuracy: 0.7397 - lr: 4.0000e-06\n"
          ]
        }
      ],
      "source": [
        "# Start to fine-tune (all layers)\n",
        "history_fine_tuning = loaded_saved_model.fit(train_data,\n",
        "                                             epochs=20, # fine-tune for a maximum of 100 epochs\n",
        "                                             steps_per_epoch=len(train_data),\n",
        "                                             validation_data=test_data,\n",
        "                                             validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n",
        "                                             callbacks=[create_tensorboard_callback(\"training_logs\", \"efficientb0_101_classes_all_data_fine_tuning\"), # track the model training logs\n",
        "                                             model_checkpoint, # save only the best model during training\n",
        "                                             early_stopping, # stop model after X epochs of no improvements\n",
        "                                             reduce_lr]) # reduce the learning rate after X epochs of no improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb5xQLjmdhv_"
      },
      "outputs": [],
      "source": [
        "## Save model to Google Drive\n",
        "loaded_saved_model.save(\"/content/drive/MyDrive/food_app/fine_tuning/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbGgyxJQdhzJ",
        "outputId": "4ed1d49c-4c05-41ec-9073-e59693706e3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "790/790 [==============================] - 57s 71ms/step - loss: 0.9540 - accuracy: 0.7402\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.9540340304374695, 0.740198016166687]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate mixed precision trained loaded model\n",
        "results_loaded_gs_model_fine_tuned = loaded_saved_model.evaluate(test_data)\n",
        "results_loaded_gs_model_fine_tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gelrVLKCdh2_",
        "outputId": "f9f823b7-b7a8-4743-a066-15ee755fb31b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "loaded_saved_model.save(\"/content/drive/MyDrive/food_app/FinalModel.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7F2e8wTspRP",
        "outputId": "8ed4a13a-9f01-4718-b4af-ddf6b3bf2b5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ],
      "source": [
        "# Load model previously saved above\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/food_app/FinalModel.hdf5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCOWAH1Omzkw"
      },
      "source": [
        "## Predicting User Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcT9C2vAmzk1"
      },
      "outputs": [],
      "source": [
        "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
        "  \"\"\"\n",
        "  Reads in an image from filename, turns it into a tensor and reshapes into\n",
        "  (224, 224, 3).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename (str): string filename of target image\n",
        "  img_shape (int): size to resize target image to, default 224\n",
        "  scale (bool): whether to scale pixel values to range(0, 1), default True\n",
        "  \"\"\"\n",
        "  # Read in the image\n",
        "  img = tf.io.read_file(filename)\n",
        "  # Decode it into a tensor\n",
        "  img = tf.image.decode_jpeg(img)\n",
        "  # Resize the image\n",
        "  img = tf.image.resize(img, [img_shape, img_shape])\n",
        "  if scale:\n",
        "    # Rescale the image (get all values between 0 and 1)\n",
        "    return img/255.\n",
        "  else:\n",
        "    return img\n",
        "\n",
        "def pred_plot_custom(img):\n",
        "  img = load_and_prep_image(img, scale=False)\n",
        "  # Make predictions using the model\n",
        "  pred_prob = model.predict(tf.expand_dims(img, axis=0))\n",
        "  # Get the predicted class with the highest probability\n",
        "  pred_class_index = pred_prob.argmax()\n",
        "  # pred_class = class_names[pred_class_index]\n",
        "  pred_accuracy = pred_prob[0, pred_class_index]\n",
        "  # Now, pred_class contains the predicted class label, and pred_accuracy contains its corresponding probability.\n",
        "  print(f\"Image: {img}, Predicted Class: {pred_class_index}, Accuracy: {pred_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFnNPj_Wmzk2"
      },
      "outputs": [],
      "source": [
        "pred_plot_custom(\"/content/paela.jpg\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
